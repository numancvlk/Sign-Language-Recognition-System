# Sign-Language-Recognition-System
# [TR]
## Projenin AmacÄ±
Bu projede, **MobileNetV2** modelini kullanarak kameradan alÄ±nan gerÃ§ek zamanlÄ± gÃ¶rÃ¼ntÃ¼lerdeki ASL (American Sign Language) el hareketlerini tanÄ±mayÄ± amaÃ§ladÄ±m. AyrÄ±ca gerÃ§ek zamanlÄ± olarak algÄ±lanan el hareketlerinin cÃ¼mle ÅŸeklinde yazÄ±lÄ±p karÅŸÄ±daki kiÅŸinin anlayabileceÄŸi yazÄ± diline dÃ¶nÃ¼ÅŸtÃ¼rmeyi hedefledim.

## ğŸ“¸ KullanÄ±lan Veri Seti
- Veri seti olarak **ASL Alphabet** veri setini kullandÄ±m.
- Veri seti [buradan](https://www.kaggle.com/datasets/grassknoted/asl-alphabet) indirilebilir.

## ğŸ’» KullanÄ±lan Teknolojiler
- Python 3.11.8
- OpenCV: Kameradan gerÃ§ek zamanlÄ± gÃ¶rÃ¼ntÃ¼leri yakalamak iÃ§in kullanÄ±ldÄ±.
- PyTorch: Derin Ã¶ÄŸrenme modelini eÄŸitmek iÃ§in kullanÄ±ldÄ±.
- Torchvision: Veri dÃ¶nÃ¼ÅŸÃ¼mlerini uygulamak iÃ§in kullanÄ±ldÄ±.
- MobileNetV2: Hafif ve hÄ±zlÄ± Ã§alÄ±ÅŸan CNN mimarisi. GerÃ§ek zamanlÄ± tahminler yaparken hÄ±z ve verimlilik saÄŸladÄ±ÄŸÄ± iÃ§in tercih ettim.
- MediaPipe: Elin anahtar noktalarÄ±nÄ±n Ã§Ä±karÄ±lmasÄ± iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
  
## âš™ï¸ Kurulum
GEREKLÄ° KÃœTÃœPHANELERÄ° KURUN
```bash
pip install tqdm
pip install torch torchvision torchaudio
pip install matplotlib
pip install scikit-image
pip install opencv-python
pip install numpy
pip install mediapipe
```

## ğŸ“¸ Ekran GÃ¶rÃ¼ntÃ¼leri 
| 1 | 2 | 
| :---------------------------------: | :------------------------: |
|<img width="1061" height="844" alt="1" src="https://github.com/user-attachments/assets/99c3cf68-dc9b-423c-9d73-03f7ea426dfe" />| <img width="1058" height="848" alt="2" src="https://github.com/user-attachments/assets/2fad0908-0422-4731-bdb6-26665c292bfa" />


## ğŸ“º Uygulama Videosu
â–¶ï¸ [Watch Project Video on YouTube](https://www.youtube.com/watch?v=U1FSrVVDVnc)


## BU PROJE HÄ°Ã‡BÄ°R ÅEKÄ°LDE TÄ°CARÄ° AMAÃ‡ Ä°Ã‡ERMEMEKTEDÄ°R.

# [EN]

## Project Objective
In this project, I aimed to recognize ASL (American Sign Language) hand gestures in real-time from camera footage using the MobileNetV2 model. Additionally, I aimed to convert the detected hand gestures into written sentences in real-time, so that the other person can understand them as text.

## ğŸ“¸ Dataset Used
- The dataset used is the ASL Alphabet dataset.
- The dataset can be downloaded [here](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

## ğŸ’» Technologies Used
- Python 3.11.8
- OpenCV: Used to capture real-time images from the camera.
- PyTorch: Used to train the deep learning model.
- Torchvision: Used to apply data transformations.
- MobileNetV2: Lightweight and fast CNN architecture. Chosen for its speed and efficiency during real-time predictions.
- MediaPipe: Used to extract key points of the hand.

## âš™ï¸ Installation
INSTALL THE REQUIRED LIBRARIES
```bash
pip install tqdm
pip install torch torchvision torchaudio
pip install matplotlib
pip install scikit-image
pip install opencv-python
pip install numpy
pip install mediapipe
```
## ğŸ“¸ Screenshots
| 1 | 2 | 
| :---------------------------------: | :------------------------: |
|<img width="1061" height="844" alt="1" src="https://github.com/user-attachments/assets/99c3cf68-dc9b-423c-9d73-03f7ea426dfe" />| <img width="1058" height="848" alt="2" src="https://github.com/user-attachments/assets/2fad0908-0422-4731-bdb6-26665c292bfa" />

## ğŸ“º Demo Video
â–¶ï¸ [Watch Project Video on YouTube](https://www.youtube.com/watch?v=U1FSrVVDVnc)

## THIS PROJECT IS IN NO WAY INTENDED FOR COMMERCIAL USE.
